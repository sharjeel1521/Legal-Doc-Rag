# -*- coding: utf-8 -*-
"""Copy of legal_doc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RQpk0FyRnznUyBmP1RxC5CHdSGcIj8SC
"""

import json
import re
import requests
import numpy as np
import pandas as pd
import csv
import pdfplumber
from datetime import datetime
from sentence_transformers import SentenceTransformer
from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType
from pymilvus import utility
import os
from nltk import sent_tokenize
import nltk

# Add required NLTK downloads
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

file_path = 'data.xlsx'

# Load the Excel file
xls = pd.ExcelFile(file_path)

# Read the first sheet by index and specify the columns to read
df = pd.read_excel(xls, sheet_name=xls.sheet_names[0], usecols=['Questions', 'Specific Document to test if exact answer required'])

# Display the size of the DataFrame
print(f"DataFrame size: {df.shape}")

# Constants and file paths
VECTOR_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
ABBREVIATION_DICT = {
    "CoA": "Contract of Adherence",
    "NDA": "Non-Disclosure Agreement",
    "T&C": "Terms and Conditions",
    "PP": "Price Protection"
}

# Function to expand abbreviations
def expand_abbreviations(text: str) -> str:
    for abbr, full_form in ABBREVIATION_DICT.items():
        text = text.replace(abbr, f"{abbr} ({full_form})")
    return text

# Function for Multi-turn Reference Resolution
def resolve_references(query: str, history: list) -> str:
    if "it" in query or "them" in query:
        for past_query in reversed(history):
            match = re.search(r"\b([A-Za-z]+)\b", past_query)
            if match:
                query = query.replace("it", match.group(1)).replace("them", match.group(1))
                break
    return query

# Function to extract text from a PDF using pdfplumber
def extract_text_from_pdf(pdf_path: str) -> str:
    try:
        with pdfplumber.open(pdf_path) as pdf:
            # Extract text from all pages
            return "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])
    except Exception as e:
        return f"Error extracting text from PDF: {str(e)}"

# Intelligent chunking of text with overlap for embeddings
def intelligent_chunking(text, max_length=500, overlap=50):
    """Intelligent document chunking with paragraph awareness and semantic boundaries"""
    try:
        chunks = []

        for page_text in text:
            # Split into paragraphs first
            paragraphs = [p.strip() for p in page_text.split('\n\n') if p.strip()]

            current_chunk = []
            current_length = 0

            for paragraph in paragraphs:
                # Fallback sentence splitting if NLTK fails
                try:
                    sentences = nltk.sent_tokenize(paragraph)
                except:
                    # Simple fallback sentence splitting
                    sentences = [s.strip() for s in re.split(r'[.!?]+', paragraph) if s.strip()]

                # Check if this is a heading or important section marker
                is_heading = any([
                    len(paragraph) < 50 and paragraph.isupper(),
                    paragraph.strip().endswith(':'),
                    re.match(r'^(?:ARTICLE|Section|SECTION|\d+\.)\s+\w+', paragraph),
                    re.match(r'^[IVX]+\.\s+\w+', paragraph)  # Roman numerals
                ])

                if is_heading and current_chunk:
                    # Store current chunk before heading
                    chunks.append(" ".join(current_chunk))
                    current_chunk = []
                    current_length = 0

                paragraph_sentences = []
                for sentence in sentences:
                    sentence = sentence.strip()
                    sentence_length = len(sentence)

                    # Check for semantic boundaries
                    is_boundary = any([
                        sentence.startswith('However,'),
                        sentence.startswith('Therefore,'),
                        sentence.startswith('Furthermore,'),
                        sentence.startswith('In addition,'),
                        'WHEREAS' in sentence,
                        'PROVIDED' in sentence.upper(),
                        re.search(r'\d+\.\s+\w+', sentence)  # Numbered points
                    ])

                    if current_length + sentence_length > max_length and current_chunk:
                        # Add overlap from previous chunk if possible
                        if overlap > 0 and current_chunk:
                            overlap_text = " ".join(current_chunk[-2:])  # Last 2 sentences for context
                            chunks.append(" ".join(current_chunk))
                            current_chunk = [overlap_text, sentence]
                            current_length = len(overlap_text) + sentence_length + 1
                        else:
                            chunks.append(" ".join(current_chunk))
                            current_chunk = [sentence]
                            current_length = sentence_length
                    else:
                        current_chunk.append(sentence)
                        current_length += sentence_length + 1  # +1 for space

                    if is_boundary and current_length > max_length/2:
                        # Create chunk at semantic boundary if we have enough content
                        chunks.append(" ".join(current_chunk))
                        current_chunk = []
                        current_length = 0

                # Add paragraph boundary marker if not at end
                if current_chunk and paragraph != paragraphs[-1]:
                    current_chunk.append("\n")
                    current_length += 1

            # Add any remaining content
            if current_chunk:
                chunks.append(" ".join(current_chunk))

        # Post-process chunks
        processed_chunks = []
        for chunk in chunks:
            # Clean up any extra whitespace or newlines
            chunk = re.sub(r'\s+', ' ', chunk).strip()

            # Ensure chunk has meaningful content
            if len(chunk) > 50:  # Minimum length threshold
                # Add context markers for better understanding
                context_markers = []
                if re.search(r'(?:ARTICLE|Section|\d+\.)\s+\w+', chunk):
                    context_markers.append("SECTION_START")
                if any(term in chunk.upper() for term in ['WHEREAS', 'PROVIDED', 'NOTWITHSTANDING']):
                    context_markers.append("LEGAL_CLAUSE")

                processed_chunks.append({
                    'text': chunk,
                    'length': len(chunk),
                    'sentence_count': len(sentences),
                    'context_markers': context_markers
                })

        return [chunk['text'] for chunk in processed_chunks]

    except Exception as e:
        print(f"Error in intelligent_chunking: {str(e)}")
        # Fallback to simple chunking if intelligent chunking fails
        return [text[i:i+max_length] for i in range(0, len(text), max_length)]

# Initialize Milvus Client and create schema
client = MilvusClient("milvus_demo9.db")
fields = [
    FieldSchema(name="chunk_id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="file_name", dtype=DataType.VARCHAR, max_length=255),
    FieldSchema(name="upload_date", dtype=DataType.VARCHAR, max_length=50),
    FieldSchema(name="entities", dtype=DataType.JSON)  # Added entities field
]

schema = CollectionSchema(fields=fields, description="Legal document chunks")

# Check if collection exists and handle accordingly
try:
    if client.has_collection("demo_collection"):
        print("Collection already exists. Dropping existing collection...")
        client.drop_collection("demo_collection")
        print("Creating new collection with updated schema...")

    client.create_collection(
        collection_name="demo_collection",
        schema=schema,
        dimension=384
    )

    # Create index with correct parameter format
    client.create_index(
        collection_name="demo_collection",
        index_params=[{
            "field_name": "embedding",
            "index_type": "IVF_FLAT",
            "metric_type": "COSINE",
            "params": {"nlist": 128}
        }]
    )
    print("Collection and index created successfully")

except Exception as e:
    print(f"Error handling collection: {str(e)}")
    raise

# Function to check the schema of a collection
def check_schema(collection_name):
    try:
        collection_info = utility.get_collection_info(collection_name)
        if 'fields' not in collection_info:
            print(f"No fields found in collection info for {collection_name}")
            return None

        # Create schema dictionary from fields
        schema = {
            field['name']: {
                'type': field['type'],
                'params': field['params']
            }
            for field in collection_info['fields']
        }
        return schema
    except Exception as e:
        print(f"Error retrieving schema: {str(e)}")
        return None

# Function to extract entities from a chunk
def extract_entities_from_chunk(chunk_text):
    """Extract entities from a single chunk"""
    return {
        'dates': re.findall(r'\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\b\d{4}[-/]\d{1,2}[-/]\d{1,2}', chunk_text),
        'amounts': re.findall(r'\$\s*\d+(?:,\d{3})*(?:\.\d{2})?|\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:USD|dollars)', chunk_text),
        'organizations': re.findall(r'(?:[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Inc\.|LLC|Ltd\.|Corporation|Corp\.|Company|Co\.))', chunk_text),
        'legal_refs': re.findall(r'(?:Section|Article|Clause)\s+\d+(?:\.\d+)*', chunk_text),
        'defined_terms': re.findall(r'"([^"]+)"\s+(?:shall\s+)?means?', chunk_text)
    }

# Modify insert_text_with_schema to include entity metadata
def insert_text_with_schema(chunk_id, text, file_name, collection_name="legal_chunks"):
    try:
        # Get embeddings
        embedding = VECTOR_MODEL.encode([text])[0]

        # Extract entities from chunk
        entities = extract_entities_from_chunk(text)

        # Get current date
        current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Prepare data with entities
        data = {
            'chunk_id': chunk_id,
            'embedding': embedding.tolist(),
            'text': text,
            'file_name': file_name,
            'upload_date': current_date,
            'entities': json.dumps(entities)  # Store entities as JSON
        }

        # Insert data
        try:
            client.insert(
                collection_name="demo_collection",
                data=data
            )
            if any(entities.values()):  # If any entities were found
                print(f"Chunk {chunk_id} inserted with entities: {json.dumps(entities, indent=2)}")
            else:
                print(f"Chunk {chunk_id} inserted (no entities found)")
            return True
        except Exception as insert_error:
            print(f"Failed to insert chunk {chunk_id}: {str(insert_error)}")
            return False

    except Exception as e:
        print(f"Error preparing data for chunk {chunk_id}: {str(e)}")
        return False

# Test the function with some sample data
insert_text_with_schema(1, "Sample text for insertion into Milvus.", "sample.pdf")


# Modify retrieve_relevant_chunks to handle entity-specific searches
def retrieve_relevant_chunks(query, top_k=5):
    """Retrieve and format relevant chunks with proper list handling"""
    try:
        # Encode query
        query_embedding = VECTOR_MODEL.encode([query])[0].tolist()

        # Search in Milvus
        search_results = client.search(
            collection_name="demo_collection",
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"nprobe": 10}},
            limit=top_k,
            output_fields=["id", "text", "file_name"]
        )

        # Format results properly
        formatted_chunks = []

        # Handle search results structure
        if isinstance(search_results, list) and search_results:
            results = search_results[0]  # Get first batch of results
            for result in results:
                # Handle different result formats
                if isinstance(result, dict):
                    chunk = {
                        'text': result.get('text', ''),
                        'score': float(result.get('score', 0.0)),
                        'id': str(result.get('id', 'unknown')),
                        'file_name': result.get('file_name', 'unknown')
                    }
                else:
                    # If result is not a dict, create basic chunk
                    chunk = {
                        'text': str(result),
                        'score': 0.5,
                        'id': f'chunk_{len(formatted_chunks)}',
                        'file_name': 'unknown'
                    }
                formatted_chunks.append(chunk)

        return formatted_chunks

    except Exception as e:
        print(f"Error in retrieve_relevant_chunks: {str(e)}")
        return []

# Function to interact with Ollama API and generate response
def make_ollama_request(prompt):
    """Enhanced Ollama request with better error handling"""
    try:
        response = requests.post('http://localhost:11434/api/generate',
                               json={
                                   'model': 'qwen2.5:32b',
                                   'prompt': prompt,
                                   'stream': False
                               })
        response.raise_for_status()

        # Extract the response text from Ollama's JSON response
        response_data = response.json()
        return response_data.get('response', '')

    except Exception as e:
        print(f"Error in Ollama request: {str(e)}")
        return None

# Function to generate a detailed response
def generate_response(user_question, document_text):
    prompt = f"""
    Contract: {document_text}
    Question: {user_question}
    Provide detailed responses based on the content of the contract."""
    return make_ollama_request(prompt)

def analyze_intent_with_llm(question):
    """Use LLM to analyze question intent with improved error handling"""

    intent_prompt = f"""
    Analyze this question and provide a JSON response:
    Question: {question}

    Respond only with a valid JSON object containing:
    {{
        "primary_intent": "date_query/party_query/term_query/etc",
        "sub_intent": "specific aspect being queried",
        "entities": {{
            "subject": "main topic",
            "target": "specific item sought",
            "context": "additional qualifiers"
        }},
        "question_type": "factual/analytical/comparative",
        "expected_format": "date/text/list/etc",
        "confidence": "high/medium/low"
    }}
    """

    try:
        # Get LLM response
        llm_response = make_ollama_request(intent_prompt)
        if not llm_response:
            raise Exception("No response from LLM")

        # Try to parse as JSON, with fallback
        try:
            intent_data = json.loads(llm_response)
        except json.JSONDecodeError:
            # Fallback to basic intent analysis
            intent_data = {
                "primary_intent": "unknown",
                "sub_intent": "unknown",
                "entities": {
                    "subject": question.split()[0] if question else "unknown",
                    "target": "unknown",
                    "context": "unknown"
                },
                "question_type": "unknown",
                "expected_format": "unknown",
                "confidence": "low"
            }

        # Create masked version
        mask_template = "Find {expected_format} about {subject} {target} in {context}"
        masked_question = mask_template.format(
            expected_format=intent_data.get('expected_format', 'information'),
            subject=intent_data.get('entities', {}).get('subject', 'topic'),
            target=intent_data.get('entities', {}).get('target', 'details'),
            context=intent_data.get('entities', {}).get('context', 'document')
        )

        return {
            'original_question': question,
            'intent_analysis': intent_data,
            'masked_question': masked_question
        }

    except Exception as e:
        print(f"Error in intent analysis: {str(e)}")
        # Return basic fallback analysis
        return {
            'original_question': question,
            'intent_analysis': {
                "primary_intent": "date_query" if "date" in question.lower() else "general_query",
                "confidence": "low",
                "entities": {
                    "subject": "document",
                    "target": "information",
                    "context": "contract"
                }
            },
            'masked_question': question
        }

def save_chunk_metadata_with_llm(chunk_text, intent_info):
    """Use LLM to extract and analyze chunk metadata"""

    metadata_prompt = f"""
    Analyze the following contract text chunk and extract key metadata.

    Text Chunk: {chunk_text}

    Question Intent: {json.dumps(intent_info.get('intent_analysis', {}), indent=2)}

    Task: Identify the following:
    1. Content Type (section type, purpose)
    2. Key Information (dates, parties, terms, obligations)
    3. Semantic Significance (importance, relevance)
    4. Relationship to Question Intent

    Format your response as JSON with these keys:
    {{
        "content_type": "identified type",
        "key_information": {{
            "dates": [],
            "parties": [],
            "terms": [],
            "obligations": []
        }},
        "semantic_markers": [],
        "intent_relevance": "high/medium/low",
        "reasoning": "why this chunk is relevant"
    }}
    """

    try:
        # Get LLM response for metadata analysis
        metadata_response = make_ollama_request(metadata_prompt)
        return json.loads(metadata_response)
    except Exception as e:
        print(f"Error in metadata analysis: {str(e)}")
        return {
            'error': str(e),
            'content_type': 'unknown'
        }

def process_question(user_question, contract_text, history=[], documents=[]):
    """Process question with integrated context retrieval and chunk handling"""
    try:
        # Analyze intent
        intent_info = analyze_intent_with_llm(user_question)
        print(f"\nIntent Analysis: {json.dumps(intent_info, indent=2)}")

        # Get relevant context sections
        context_sections = get_relevant_context(user_question, contract_text)

        # Get vector search chunks
        vector_chunks = retrieve_relevant_chunks(user_question, top_k=5)

        # Combine and deduplicate results
        all_chunks = []
        seen_texts = set()

        # Add context sections
        for section in context_sections:
            if section['text'] not in seen_texts:
                all_chunks.append({
                    'id': f'section_{len(all_chunks)}',
                    'text': section['text'],
                    'score': float(section['score']),
                    'source': 'context_retrieval',
                    'metadata': {
                        'content_type': 'text',
                        'relevance': 'high' if section['score'] > 0.7 else 'medium'
                    }
                })
                seen_texts.add(section['text'])

        # Add vector chunks
        for chunk in vector_chunks:
            if chunk['text'] not in seen_texts:
                all_chunks.append({
                    'id': chunk['id'],
                    'text': chunk['text'],
                    'score': chunk['score'],
                    'source': 'vector_search',
                    'metadata': {
                        'content_type': 'text',
                        'relevance': 'high' if chunk['score'] > 0.7 else 'medium',
                        'file_name': chunk.get('file_name', 'unknown')
                    }
                })
                seen_texts.add(chunk['text'])

        # Sort all chunks by score
        all_chunks.sort(key=lambda x: x['score'], reverse=True)

        if not all_chunks:
            return json.dumps({
                "status": "no_results",
                "message": "No relevant information found",
                "question": user_question,
                "intent_analysis": intent_info
            }, indent=4)

        # Prepare enhanced context
        context = "\n\n".join([
            f"[{chunk['source'].upper()} Chunk {i+1}] (Score: {chunk['score']:.2f})\n"
            f"Source: {chunk['metadata'].get('file_name', 'context')}\n"
            f"Relevance: {chunk['metadata']['relevance']}\n"
            f"Content: {chunk['text'][:200]}..."
            for i, chunk in enumerate(all_chunks[:5])  # Use top 5 chunks
        ])

        # Prepare prompt with intent and combined context
        answer_prompt = f"""
        Question: {user_question}
        Intent Analysis: {json.dumps(intent_info['intent_analysis'], indent=2)}

        Combined Relevant Sections:
        {context}

        Based on the above sections, provide a direct answer to the question.
        Format your response as JSON:
        {{
            "answer": "clear and concise answer",
            "confidence": "high/medium/low",
            "reasoning": "brief explanation",
            "source_chunks": ["chunk sources and IDs used"],
            "supporting_text": "relevant quotes from chunks"
        }}
        """

        # Get LLM response
        llm_response = make_ollama_request(answer_prompt)

        if not llm_response:
            return json.dumps({
                "status": "no_llm_response",
                "answer": "Unable to generate response",
                "confidence": "low",
                "question": user_question,
                "intent_analysis": intent_info,
                "chunks_found": len(all_chunks)
            }, indent=4)

        try:
            response_data = json.loads(llm_response)
            response_data.update({
                "status": "success",
                "intent_analysis": intent_info,
                "chunks_found": {
                    "total": len(all_chunks),
                    "context": len(context_sections),
                    "vector": len(vector_chunks)
                },
                "top_chunk_score": all_chunks[0]['score'] if all_chunks else 0
            })
        except json.JSONDecodeError:
            response_data = {
                "status": "partial_success",
                "answer": llm_response[:500],
                "confidence": "low",
                "reasoning": "Response format error, but content provided",
                "chunks_found": len(all_chunks),
                "intent_analysis": intent_info
            }

        return json.dumps(response_data, indent=4)

    except Exception as e:
        print(f"Error in process_question: {str(e)}")
        return json.dumps({
            "status": "error",
            "message": str(e),
            "question": user_question,
            "intent_analysis": intent_info if 'intent_info' in locals() else None,
            "chunks_found": len(all_chunks) if 'all_chunks' in locals() else 0
        }, indent=4)


# Limit processing to 2 documents
output_file = 'output.csv'
with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Question', 'PDF Path', 'Response'])

    # Process only first 2 rows
    for index, row in df.head(2).iterrows():
        print(f"\nProcessing Document {index + 1}")
        #question = row['Questions']
        pdf_path = row['Specific Document to test if exact answer required']
        print(f"Question: {question}")
        print(f"Document: {pdf_path}")

        # Extract text from the PDF
        print("Extracting text from PDF...")
        contract_text = extract_text_from_pdf(pdf_path)

        # Intelligent chunking and insertion
        print("Chunking document and inserting into Milvus...")
        chunks = intelligent_chunking(contract_text)
        for idx, chunk in enumerate(chunks):
            insert_text_with_schema(idx, chunk, pdf_path)

        # Process question and get response
        print("Processing question and generating response...")
        response = process_question(question, contract_text, [], [])
        print("\nResponse:")
        print(response)

        # Write to CSV
        writer.writerow([question, pdf_path, response])
        print(f"Results written for document {index + 1}")

print(f"\nResults have been written to {output_file}")

def insert_chunks_to_db(contract_text, pdf_path):
    """Insert document chunks into Milvus DB using recursive chunking and key phrase extraction"""
    try:
        print("Processing document for insertion...")

        # Convert text to pages format and get page count
        text_per_page, page_count = (
            contract_text if isinstance(contract_text, tuple)
            else (contract_text.split('\n\n'), 1)
        )

        # Use recursive chunking
        chunks = recursive_chunking(text_per_page, max_length=500)
        print(f"Created {len(chunks)} chunks using recursive chunking")

        # Extract key phrases from full text
        full_text = " ".join(text_per_page)
        key_phrases = extract_key_phrases(full_text)
        print(f"Extracted key phrases: {key_phrases}")

        # Prepare metadata
        metadata = {
            "file_name": os.path.basename(pdf_path),
            "page_count": page_count,
            "num_chunks": len(chunks),
            "key_phrases": key_phrases
        }
        print("Metadata:", metadata)

        # Get embeddings using sentence transformer
        model = SentenceTransformer("all-MiniLM-L6-v2")
        embeddings = model.encode(chunks)
        embeddings_array = np.array(embeddings, dtype=np.float32)

        # Prepare data for insertion with chunk-specific key phrases
        data = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings_array)):
            # Extract key phrases for each chunk
            chunk_key_phrases = extract_key_phrases(chunk)

            data.append({
                "id": i,
                "vector": embedding.tolist(),
                "file_name": metadata["file_name"],
                "page_count": metadata["page_count"],
                "num_chunks": metadata["num_chunks"],
                "key_phrases": chunk_key_phrases,  # Chunk-specific key phrases
                "text": chunk,  # Store the chunk text
                "doc_key_phrases": metadata["key_phrases"]  # Store document-level key phrases
            })

        # Insert into Milvus
        try:
            client.insert(collection_name=collection_name, data=data)
            print(f"Successfully inserted {len(data)} chunks with metadata")
            return chunks
        except Exception as insert_error:
            print(f"Failed to insert chunks: {str(insert_error)}")
            return []

    except Exception as e:
        print(f"Error in insert_chunks_to_db: {str(e)}")
        return []

def retrieve_relevant_chunks(question, top_k=5):
    """Retrieve and rank relevant chunks using improved similarity search"""
    try:
        # Encode question
        model = SentenceTransformer("all-MiniLM-L6-v2")
        query_embedding = model.encode([question])[0]

        # Extract key phrases from question
        question_key_phrases = extract_key_phrases(question)
        print(f"Question key phrases: {question_key_phrases}")

        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 10}
        }

        # Initial similarity search
        search_results = client.search(
            collection_name="demo_collection",
            data=[query_embedding.tolist()],
            anns_field="vector",
            search_params=search_params,
            limit=top_k * 2,  # Get more results for reranking
            output_fields=["id", "file_name", "text", "key_phrases", "doc_key_phrases"]
        )


        if not search_results or len(search_results[0]) == 0:
            return []

        # Rerank results based on multiple factors
        reranked_results = []
        for result_list in search_results:
            for result in result_list:
                chunk_text = result.get('text', '')
                chunk_key_phrases = result.get('key_phrases', '')
                doc_key_phrases = result.get('doc_key_phrases', '')

                # Calculate various scores
                score = {
                    'similarity': float(result.score),
                    'chunk_keyword_match': len(set(question_key_phrases.split(', ')) &
                                         set(chunk_key_phrases.split(', '))),
                    'doc_keyword_match': len(set(question_key_phrases.split(', ')) &
                                        set(doc_key_phrases.split(', '))),
                    'paragraph_coherence': len(sent_tokenize(chunk_text)),
                    'content_length': min(len(chunk_text.split()) / 500, 1.0)
                }

                # Calculate combined score with weights
                combined_score = (
                    score['similarity'] * 0.4 +
                    score['chunk_keyword_match'] * 0.25 +
                    score['doc_keyword_match'] * 0.15 +
                    score['paragraph_coherence'] * 0.1 +
                    score['content_length'] * 0.1
                )

                reranked_results.append({
                    'chunk_id': result.get('id'),
                    'text': chunk_text,
                    'score': combined_score,
                    'metadata': {
                        'file_name': result.get('file_name'),
                        'chunk_key_phrases': chunk_key_phrases,
                        'doc_key_phrases': doc_key_phrases
                    }
                })

        # Sort by combined score and get top_k
        reranked_results.sort(key=lambda x: x['score'], reverse=True)
        return reranked_results[:top_k]

    except Exception as e:
        print(f"Error in retrieve_relevant_chunks: {str(e)}")
        return []

def process_question(user_question, contract_text, history=[], documents=[]):
    """Process question using improved chunk retrieval and LLM"""
    try:
        # Get reranked chunks
        relevant_chunks = retrieve_relevant_chunks(user_question, top_k=5)

        if not relevant_chunks:
            return json.dumps({
                "status": "error",
                "message": "No relevant chunks found",
                "question": user_question,
                "confidence": "none"
            }, indent=4)

        # Prepare context with key phrases
        context = "\n\n".join([
            f"[Chunk {i+1} (score: {chunk['score']:.3f})]\n"
            f"Key phrases: {chunk['metadata']['chunk_key_phrases']}\n"
            f"Text: {chunk['text']}"
            for i, chunk in enumerate(relevant_chunks)
        ])

        # Enhanced prompt for LLM
        prompt = f"""
        Question: {user_question}

        Document Context:
        {context}

        Task: Based on the provided context, answer the question. Consider:
        1. Relevance of each chunk (indicated by score)
        2. Key phrases in each chunk
        3. Document context and relationships
        4. Specific details and dates if present

        If multiple possible answers exist, explain why you chose a particular one.
        If the answer isn't clear from the context, say so.

        Format your response as JSON with these keys:
        {{
            "answer": "your detailed answer",
            "confidence": "high/medium/low",
            "reasoning": "explanation of your answer",
            "source_chunks": ["chunk numbers used"],
            "key_phrases_used": ["relevant key phrases that led to the answer"]
        }}
        """

        # Get LLM response
        llm_response = make_ollama_request(prompt)

        try:
            response_data = json.loads(llm_response)
            response_data["relevant_chunks"] = relevant_chunks
            return json.dumps(response_data, indent=4)
        except json.JSONDecodeError:
            return json.dumps({
                "status": "error",
                "message": "Failed to parse LLM response",
                "raw_response": llm_response
            }, indent=4)

    except Exception as e:
        print(f"Error in process_question: {str(e)}")
        return json.dumps({
            "status": "error",
            "message": str(e),
            "question": user_question
        }, indent=4)